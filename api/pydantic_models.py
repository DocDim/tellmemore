from pydantic import BaseModel, Field, field_validator, ValidationError
from enum import Enum
from datetime import datetime, timezone
from typing import Optional, List
import uuid

class ModelProvider(str, Enum):
    OPENAI = "openai"
    GOOGLE = "google"
    GROQ = "groq"

class ModelName(str, Enum):
    """
    Enum representing supported language models across multiple providers.

    Models are categorized by provider:
    - OpenAI: GPT-4 series
    - Google: Gemini models
    - Groq Cloud: LLaMA3 variants

    This enum enables strong validation and consistency when referring to models
    throughout the application.

    Attributes:
        GPT_4O: OpenAI's GPT-4o model
        GPT_4O_MINI: Hypothetical variant of GPT-4o (e.g., smaller version)
        GEMINI_1_5_PRO_LATEST: Latest Google Gemini 1.5 Pro model
        GEMINI_1_5_FLASH_LATEST: Latest Google Gemini 1.5 Flash model
        GEMINI_1_0_PRO: Google Gemini 1.0 Pro model
        LLAMA3_8B_8192: Groq's hosted LLaMA3 8B model
        LLAMA3_70B_8192: Groq's hosted LLaMA3 70B model
    """
    # OpenAI Models
    GPT_4O = "gpt-4o"
    GPT_4O_MINI = "gpt-4o-mini"

    # Google Models
    GEMINI_1_5_PRO_LATEST = "gemini-1.5-pro-latest"
    GEMINI_1_5_FLASH_LATEST = "gemini-1.5-flash-latest"
    GEMINI_1_0_PRO = "gemini-1.0-pro"

    # Groq Cloud Models (still defined, but not used in every comparison)
    LLAMA3_8B_8192 = "llama3-8b-8192"
    LLAMA3_70B_8192 = "llama3-70b-8192"

    def get_provider(self) -> ModelProvider:
        """
        Returns the provider associated with the model name.

        This helper method infers the provider (OpenAI, Google, Groq)
        by inspecting the model's value. This is useful for dynamically
        routing model usage logic based on source.

        Returns:
            ModelProvider: The corresponding provider enum.

        Raises:
            ValueError: If the provider cannot be determined from the model name.
        """
        value_lower = self.value.lower()
        if value_lower.startswith("gpt"):
            return ModelProvider.OPENAI
        elif value_lower.startswith("gemini"):
            return ModelProvider.GOOGLE
        elif "llama3" in value_lower: # Simplified for the Groq models we have
            return ModelProvider.GROQ
        raise ValueError(f"Could not determine provider for model: {self.value}")


# Helper model for chat history input in the API
class ChatMessageAPI(BaseModel):
    role: str = Field(..., examples=["user", "assistant"])
    content: str

    @field_validator('role')
    @classmethod
    def role_must_be_valid(cls, v: str) -> str:
        role_lower = v.lower()
        if role_lower not in ["user", "assistant", "system", "human", "ai"]: # Allow more for flexibility
            raise ValueError("Role must be one of 'user', 'assistant', 'system', 'human', 'ai'")
        return role_lower
    
# class QueryInputForComparison(BaseModel):
#     """
#     Request model for submitting a query to the LLM comparison endpoint.

#     This model is used to encapsulate the user's input question and an optional session ID.
#     The session ID allows grouping multiple requests into a single logical session for
#     tracking, analysis, or UI display purposes.

#     Attributes:
#         question (str): The input question or prompt to be compared across different LLM providers.
#         session_id (Optional[str]): A unique identifier for the session. If not provided, a new UUID
#                                     is automatically generated to uniquely identify the request context.
#     """
#     question: str
#     session_id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()))
    
#     @field_validator('session_id', mode='before') # (B)
#     @classmethod
#     def set_session_id_if_none(cls, v):
#         return v or str(uuid.uuid4())

# Request body for individual LLM chat endpoints
class SingleModelChatRequest(BaseModel):
    question: str = Field(..., examples=["What is the capital of France?"])
    session_id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()))
    chat_history: Optional[List[ChatMessageAPI]] = Field(
        default_factory=list,
        examples=[[{"role": "user", "content": "My name is Bob."}]]
    )

    @field_validator('session_id', mode='before')
    @classmethod
    def set_session_id_if_none(cls, v: Optional[str]) -> str:
        return v or str(uuid.uuid4())

class QueryResponse(BaseModel):
    """
    Represents a single response returned by an AI model in a comparison session.

    This model is used to structure the metadata and result of a model's response to a query.
    It includes timing information, the model identity, and the associated session ID.

    Attributes:
        answer (str): The textual response generated by the AI model.
        session_id (str): Unique identifier for the comparison session this response belongs to.
        model (ModelName): The specific model that generated the response (e.g., GPT-4o, Gemini).
        provider (ModelProvider): The provider associated with the model (e.g., OpenAI, Google, Groq).
        request_timestamp (datetime): The timestamp when the query was sent to the model.
        response_timestamp (datetime): The timestamp when the response was received from the model.
        latency_ms (float): The total time taken to receive the model's response, in milliseconds.
    """
    answer: Optional[str] = None
    error_message: Optional[str] = None
    session_id: str
    model: ModelName
    provider: ModelProvider
    request_timestamp: datetime
    response_timestamp: datetime
    latency_ms: float
    
    @field_validator('request_timestamp', 'response_timestamp', mode='before')
    @classmethod
    def ensure_datetime_is_aware(cls, v: datetime) -> datetime: # Use `any` for `v` if it might be a string from JSON
        """
        Validates that the datetime field is timezone-aware.
        This runs AFTER Pydantic has converted the input to a datetime object.
        """
        if v.tzinfo is None:
            raise ValueError("Datetime must be timezone-aware (UTC).")
        return v.astimezone(timezone.utc)
    

class ComparisonResponse(BaseModel):
    """
    Represents the collection of responses from multiple language models for a single input query.

    This model is used to return all model responses for a given question, grouped under a shared
    session ID. It includes metadata such as the original user question and the timestamp when the
    comparison was performed.

    Attributes:
        original_question (str): The user-provided question that was submitted to all models.
        session_id (str): Unique identifier used to group all model responses under a single session.
        responses (List[QueryResponse]): A list of individual responses from each model involved in the comparison.
        comparison_timestamp (datetime): The timestamp when the comparison was completed. Defaults to the current UTC time.
    """
    original_question: str
    session_id: str
    responses: List[QueryResponse]
    comparison_timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))